{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RLC.move_chess.environment import Board\n",
    "from RLC.move_chess.agent import Piece\n",
    "from RLC.move_chess.learn import Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[S]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[F]', '[ ]', '[ ]']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Board()\n",
    "env.render()\n",
    "env.visual_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Piece(piece='king')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Reinforce(p,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def monte_carlo_learning(self, epsilon=0.1):\n",
      "        state = (0, 0)  # Estado inicial\n",
      "        self.env.state = state  # Configura el estado inicial del entorno\n",
      "\n",
      "        states, actions, rewards = self.play_episode(state, epsilon=epsilon)  # Juega un episodio y obtiene los estados, acciones y recompensas\n",
      "        \n",
      "        first_visits = []  # Almacena los estados y acciones visitados por primera vez en el episodio\n",
      "        for idx, state in enumerate(states):\n",
      "            action_index = actions[idx]  # Obtiene el índice de la acción correspondiente al estado actual\n",
      "            if (state, action_index) in first_visits:  # Si ya se ha visitado este estado y acción, continúa al siguiente paso\n",
      "                continue\n",
      "            r = np.sum(rewards[idx:])  # Calcula la recompensa total acumulada a partir de este estado y acción en adelante\n",
      "            if (state, action_index) in self.agent.Returns.keys():  # Si ya hay registros de retornos para este estado y acción\n",
      "                self.agent.Returns[(state, action_index)].append(r)  # Agrega la recompensa a la lista de retornos\n",
      "            else:  # Si es la primera vez que se encuentra este estado y acción\n",
      "                self.agent.Returns[(state, action_index)] = [r]  # Crea una nueva entrada en los retornos\n",
      "            self.agent.action_function[state[0], state[1], action_index] = np.mean(self.agent.Returns[(state, action_index)])  # Actualiza la función de acción tomando el promedio de los retornos acumulados\n",
      "            first_visits.append((state, action_index))  # Registra que este estado y acción han sido visitados por primera vez en el episodio\n",
      "\n",
      "        self.agent.policy = self.agent.action_function.copy()  # Actualiza la política a partir de la función de acción\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(r.monte_carlo_learning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "    eps = 0.5\n",
    "    r.monte_carlo_learning(epsilon=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['→', '↖', '↘', '↓', '←', '↑', '←', '→'],\n",
      " ['↓', '↖', '↓', '←', '↘', '↘', '↘', '↘'],\n",
      " ['↓', '↓', '↘', '↙', '↑', '↗', '↙', '←'],\n",
      " ['↑', '↙', '↙', '↓', '↓', '↗', '↙', '→'],\n",
      " ['↙', '↓', '←', '↙', '↓', '↘', '→', '↑'],\n",
      " ['↘', '→', '→', '↓', '↘', '↘', '↙', '↗'],\n",
      " ['→', '→', '↘', '↓', '↘', '↓', '↙', '→'],\n",
      " ['↙', '↑', '↗', '↗', '→', 'F', '←', '↖']]\n"
     ]
    }
   ],
   "source": [
    "r.visualize_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def sarsa_td(self, n_episodes=1000, alpha=0.01, gamma=0.9):\n",
      "        for k in range(n_episodes):\n",
      "            state = (0, 0)  # Estado inicial\n",
      "            self.env.state = state  # Configura el estado inicial del entorno\n",
      "            episode_end = False  # Indica si el episodio ha terminado o no\n",
      "            epsilon = max(1 / (1 + k), 0.05)  # Calcula el valor de epsilon para la política epsilon-greedy\n",
      "            \n",
      "            while not episode_end:\n",
      "                state = self.env.state  # Obtiene el estado actual\n",
      "                action_index = self.agent.apply_policy(state, epsilon)  # Aplica la política epsilon-greedy para seleccionar una acción\n",
      "                action = self.agent.action_space[action_index]  # Obtiene la acción correspondiente al índice seleccionado\n",
      "                reward, episode_end = self.env.step(action)  # Realiza un paso en el entorno y obtiene la recompensa y el indicador de fin de episodio\n",
      "                successor_state = self.env.state  # Obtiene el estado sucesor después de realizar la acción\n",
      "                successor_action_index = self.agent.apply_policy(successor_state, epsilon)  # Aplica la política epsilon-greedy al estado sucesor\n",
      "\n",
      "                action_value = self.agent.action_function[state[0], state[1], action_index]  # Valor de la acción actual en el estado actual\n",
      "                successor_action_value = self.agent.action_function[successor_state[0], successor_state[1], successor_action_index]  # Valor de la acción sucesora en el estado sucesor\n",
      "\n",
      "                q_update = alpha * (reward + gamma * successor_action_value - action_value)  # Cálculo de la actualización Q\n",
      "\n",
      "                self.agent.action_function[state[0], state[1], action_index] += q_update  # Actualización del valor Q de la acción actual\n",
      "                self.agent.policy = self.agent.action_function.copy()  # Actualiza la política a partir de la función de acción\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(r.sarsa_td))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Piece(piece='king')\n",
    "env = Board()\n",
    "r = Reinforce(p,env)\n",
    "r.sarsa_td(n_episodes=100,alpha=0.2,gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['↓', '→', '↗', '↓', '↑', '↓', '↓', '↘'],\n",
      " ['↘', '↗', '↓', '→', '←', '↙', '↖', '↑'],\n",
      " ['↖', '↓', '↘', '↗', '↑', '↘', '→', '↗'],\n",
      " ['↖', '↘', '↑', '→', '↘', '←', '←', '↓'],\n",
      " ['↖', '↓', '↖', '←', '↑', '↘', '↓', '↗'],\n",
      " ['←', '↖', '↘', '↘', '↑', '↓', '↓', '↗'],\n",
      " ['↙', '↙', '←', '→', '↘', '↓', '↙', '→'],\n",
      " ['↘', '↘', '↓', '↗', '→', 'F', '←', '←']]\n"
     ]
    }
   ],
   "source": [
    "r.visualize_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def sarsa_lambda(self, n_episodes=1000, alpha=0.05, gamma=0.9, lamb=0.8):\n",
      "        for k in range(n_episodes):\n",
      "            self.agent.E = np.zeros(shape=self.agent.action_function.shape)  # Inicializa la traza de elegibilidad a cero\n",
      "            state = (0, 0)  # Estado inicial\n",
      "            self.env.state = state  # Configura el estado inicial del entorno\n",
      "            episode_end = False  # Indica si el episodio ha terminado o no\n",
      "            epsilon = max(1 / (1 + k), 0.2)  # Calcula el valor de epsilon para la política epsilon-greedy\n",
      "            action_index = self.agent.apply_policy(state, epsilon)  # Aplica la política epsilon-greedy para seleccionar una acción\n",
      "            action = self.agent.action_space[action_index]  # Obtiene la acción correspondiente al índice seleccionado\n",
      "            \n",
      "            while not episode_end:\n",
      "                reward, episode_end = self.env.step(action)  # Realiza un paso en el entorno y obtiene la recompensa y el indicador de fin de episodio\n",
      "                successor_state = self.env.state  # Obtiene el estado sucesor después de realizar la acción\n",
      "                successor_action_index = self.agent.apply_policy(successor_state, epsilon)  # Aplica la política epsilon-greedy al estado sucesor\n",
      "\n",
      "                action_value = self.agent.action_function[state[0], state[1], action_index]  # Valor de la acción actual en el estado actual\n",
      "                if not episode_end:\n",
      "                    successor_action_value = self.agent.action_function[successor_state[0], successor_state[1], successor_action_index]  # Valor de la acción sucesora en el estado sucesor\n",
      "                else:\n",
      "                    successor_action_value = 0  # Valor 0 si el episodio ha terminado\n",
      "\n",
      "                delta = reward + gamma * successor_action_value - action_value  # Diferencia temporal de los valores Q\n",
      "                self.agent.E[state[0], state[1], action_index] += 1  # Incrementa la traza de elegibilidad en la posición correspondiente\n",
      "                self.agent.action_function = self.agent.action_function + alpha * delta * self.agent.E  # Actualiza la función de acción con la traza de elegibilidad\n",
      "                self.agent.E = gamma * lamb * self.agent.E  # Decrementa la traza de elegibilidad de acuerdo con el factor de descuento y la tasa de decaimiento lambda\n",
      "                state = successor_state  # Actualiza el estado actual con el estado sucesor\n",
      "                action = self.agent.action_space[successor_action_index]  # Actualiza la acción actual con la acción sucesora\n",
      "                action_index = successor_action_index  # Actualiza el índice de acción actual con el índice de acción sucesora\n",
      "                self.agent.policy = self.agent.action_function.copy()  # Actualiza la política a partir de la función de acción\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(r.sarsa_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Piece(piece='king')\n",
    "env = Board()\n",
    "r = Reinforce(p,env)\n",
    "r.sarsa_lambda(n_episodes=100,alpha=0.2,gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['↓', '↙', '↑', '↙', '↗', '←', '↑', '↙'],\n",
      " ['↓', '↓', '↗', '←', '↘', '↘', '→', '←'],\n",
      " ['↘', '↘', '↓', '↓', '↙', '↙', '↓', '↖'],\n",
      " ['↘', '↘', '↘', '↓', '←', '←', '↙', '↙'],\n",
      " ['↖', '↘', '↘', '↘', '←', '↑', '↘', '↙'],\n",
      " ['↗', '↖', '↖', '↘', '↓', '↑', '↙', '↓'],\n",
      " ['↑', '↓', '↗', '→', '↘', '↓', '↙', '↗'],\n",
      " ['↙', '↘', '↗', '↗', '↘', 'F', '←', '↖']]\n"
     ]
    }
   ],
   "source": [
    "r.visualize_policy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
